{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slhoefel/notebooks/blob/main/scrape_WHO_news.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from time import sleep\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = ('/content/drive/MyDrive/WHO')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlcGkYiJ4tUu",
        "outputId": "9230db1c-69a6-4575-d380-2c0d7fe63657"
      },
      "id": "SlcGkYiJ4tUu",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script retrieves reports from WHO's Disease Outbreaks Newsfeed. Links from each page are extracted and stored as a text file by the first function \"get_urls.\"\n",
        "\n",
        "To retrieve multiple pages at once, loop the \"get_pages\" function through the rage of pages you want to download"
      ],
      "metadata": {
        "id": "jVNjYgsbLdUJ"
      },
      "id": "jVNjYgsbLdUJ"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "59234d38-0380-44f3-a310-357927551674",
      "metadata": {
        "id": "59234d38-0380-44f3-a310-357927551674"
      },
      "outputs": [],
      "source": [
        "#p=page of WHO DON feed to search\n",
        "def get_urls(p):\n",
        "    url = f'https://www.who.int/en/emergencies/disease-outbreak-news/{str(p)}'\n",
        "    grab = requests.get(url)\n",
        "    soup = BeautifulSoup(grab.text, 'html.parser')\n",
        "\n",
        "    # open a file in write mode\n",
        "    f = open(f\"{path}/pages/WHO_page_{p}.txt\", \"w\")\n",
        "    # search paragraphs from soup\n",
        "    for link in soup.find_all('a',class_= 'sf-list-vertical__item'):\n",
        "        #extract url\n",
        "        data = link.get('href')\n",
        "        f.write(data)\n",
        "        f.write(\"\\n\")\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir $path/pages/\n",
        "!mkdir $path/urls/\n",
        "!mkdir $path/downloaded_articles\n",
        "for n in range(1996,2024):\n",
        "    !mkdir $path/downloaded_articles/$n"
      ],
      "metadata": {
        "id": "CFrsH5SJBIZu"
      },
      "id": "CFrsH5SJBIZu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "a2838a1f-979c-486f-a954-db237f84de62",
      "metadata": {
        "id": "a2838a1f-979c-486f-a954-db237f84de62"
      },
      "outputs": [],
      "source": [
        "#use get_urls() for single page, use get_pages() for multiple pages\n",
        "\n",
        "#retrieve by page\n",
        "all_pages = []\n",
        "def get_pages(p):\n",
        "    get_urls(p)\n",
        "    f = open(f'{path}/pages/WHO_page_{p}.txt')\n",
        "    urls = [x.strip('\\n') for x in f.readlines()]\n",
        "    all_pages.extend(urls)\n",
        "#there are currently 142 pages of DON newsfeed results\n",
        "for n in range(1,143):\n",
        "    get_pages(n)\n",
        "\n",
        "#save all downloaded urls from all pages in a time-stamped text file for documentation purposes\n",
        "now = datetime.now().strftime(\"%d:%B:%Y_%H:%M:%S\")\n",
        "with open(f'{path}/urls/all_{now}.txt', 'w') as f:\n",
        "    f.writelines([x +'\\n'for x in all_pages][:-1]+[[x for x in all_pages][-1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "49fad332-e98f-4c15-a5de-aff78824f6f5",
      "metadata": {
        "tags": [],
        "id": "49fad332-e98f-4c15-a5de-aff78824f6f5"
      },
      "outputs": [],
      "source": [
        "#import and simplify country names to match article locations with ISO codes in ISO key\n",
        "def arrange(x):\n",
        "    x = x.split(', ')\n",
        "    x = x[1]+' '+x[0]\n",
        "    x = x.replace('Province of China ','')\n",
        "    return x\n",
        "\n",
        "ISO = pd.read_csv(f'{path}/ISO.csv')[['name','alpha-3']]\n",
        "ISO.name = ISO.name.apply(lambda x: arrange(x.split('(')[0]) if (', ' in x) and (' and ' not in x) else x.split(' (')[0])\n",
        "ISO.name = ISO.name.apply(lambda x: x.split(' of America')[0].split(' of Great')[0].split('n Federation')[0])\n",
        "iso_dict = dict(zip(ISO['name'],ISO['alpha-3']))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once urls are downloaded and stored, they can be passed to the \"get_articles\" fuction. This function uses bs4 (BeautifulSoup) to extract data from the linked page by HTML tags"
      ],
      "metadata": {
        "id": "3bzZrUA_MgjB"
      },
      "id": "3bzZrUA_MgjB"
    },
    {
      "cell_type": "code",
      "source": [
        "def next_key(key,temp):\n",
        "  #get next key in order of appearance to create section boundary\n",
        "  try:\n",
        "      next = temp[temp.index(key) + 1]\n",
        "  #no further headings\n",
        "  except (ValueError, IndexError):\n",
        "      next = None\n",
        "  return next"
      ],
      "metadata": {
        "id": "IduI4PonSFDr"
      },
      "id": "IduI4PonSFDr",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "68906e82-453b-4dc8-bdba-96898ffff785",
      "metadata": {
        "id": "68906e82-453b-4dc8-bdba-96898ffff785"
      },
      "outputs": [],
      "source": [
        "#retrieve text from DON report with the article url\n",
        "def get_article(url):\n",
        "    #print for troubleshooting\n",
        "    print(url)\n",
        "    #retrieve article\n",
        "    grab = requests.get(url)\n",
        "    #read text as html\n",
        "    soup = BeautifulSoup(grab.text, 'html.parser')\n",
        "\n",
        "    #get date from <timestamp> html tag\n",
        "    s = soup.find('span',class_= 'timestamp')\n",
        "    date = s.get_text()\n",
        "\n",
        "    #get report title from <h1> tag within <div class='sf-item-header-wrapper'> division\n",
        "    s = soup.find('div', class_=\"sf-item-header-wrapper\")\n",
        "    title = s.h1.text.replace('Saint-','Saint ')\n",
        "    title = title.replace('â€“','-').replace('\\n','')\n",
        "\n",
        "    #split title into disease and country names\n",
        "    what,where = title.split('-')[:-1],title.split('-')[-1].strip()\n",
        "    #rejoin diseases names like \"COVID-19\" that are split by disease-country separation\n",
        "    if type(what)==list:\n",
        "        what = '-'.join(what)\n",
        "    #occassionally disease is listed second instead of first\n",
        "    if where.strip().count(' ') > 5:\n",
        "        if ((any(x.lower() in what.lower() for x in iso_dict.keys())) or ('multi-country' in what.lower())) and \\\n",
        "         ((all(x.lower() not in where.lower() for x in iso_dict.keys())) and ((all(x.lower() not in where.lower() for x in ['multi-country','Europe','America','Asia','Africa','Pacific'])))):\n",
        "            where = what\n",
        "            what = where\n",
        "\n",
        "    #open write file\n",
        "    f = open(f\"{path}/WHO_report_text.txt\", \"w\")\n",
        "    #get main text with <article class='sf-detail-body-wrapper> tag\n",
        "    body = soup.find('article',class_='sf-detail-body-wrapper')\n",
        "    #use multiple searches for headers due to inconsistent sectioning\n",
        "    #sometimes <h3> headers, sometimes single-line paragraphs with <strong> tag\n",
        "    #<h5 class=\"section_head3\">Situation update: </h5> in earlier posts\n",
        "    headers = [x.text for x in body.findChildren('strong')] + [x.text for x in body.findChildren('h3')] + [x.text for x in body.findChildren('h5',{'class':'section_head3'})]\n",
        "    headers = [x.replace('*','').split(':')[0].split('(')[0] for x in headers if len(x)>5]\n",
        "    body = body.text\n",
        "    #sort headers by order of appearance by searching text\n",
        "    headers = re.findall('|'.join(headers),body)\n",
        "    #write to file\n",
        "    f.write(body)\n",
        "    f.close()\n",
        "\n",
        "    #break article into sections\n",
        "    #section names and order are inconsistent, so many conditional statements are needed to organize the many different configurations\n",
        "\n",
        "    sections = {}\n",
        "    for header in headers:\n",
        "        if 'at a glance' in header:\n",
        "            sections.update({'summary':header})\n",
        "        if 'Situation' in header:\n",
        "            sections.update({'summary':header})\n",
        "        if 'Description of' in header:\n",
        "            sections.update({'background':header})\n",
        "        if 'Public health' in header:\n",
        "            sections.update({'response':header})\n",
        "        if 'Epidemiology' in header:\n",
        "            sections.update({'epidemiology':header})\n",
        "\n",
        "    f = open(f'{path}/WHO_report_text.txt')\n",
        "    text_body = f.read()\n",
        "    f.close()\n",
        "\n",
        "    if len([v for v in sections.values() if v != 'None']) > 0:\n",
        "\n",
        "        for k,v in sections.items():\n",
        "            section_break = next_key(v,headers)\n",
        "            if v == 'None':\n",
        "                continue\n",
        "            elif section_break == None:\n",
        "              try:\n",
        "                  section_text = re.search(f\"(?s){v}(.*$)\",text_body)[0].replace(v,f'{v}\\n')\n",
        "              except Exception:\n",
        "                  section_text = text_body.split(v)[-1]\n",
        "            else:\n",
        "              try:\n",
        "                section_text = re.search(f\"(?s){v}(.*?){section_break}\",text_body)[0].replace(v,f'{v}\\n').replace(section_break,'')\n",
        "              except Exception:\n",
        "                  section_text = text_body.split(v)[-1].split(section_break)[0]\n",
        "            sections.update({k:section_text})\n",
        "\n",
        "        if 'summary' not in sections.keys():\n",
        "            section_break = [x for x in sections.values() if x != 'None'][0].split('\\n')[0]\n",
        "            try:\n",
        "                sections.update({'summary':text_body.split(section_break)[0]})\n",
        "            except ValueError:\n",
        "                sections.update({'summary':text_body})\n",
        "\n",
        "    #if sections missing, unidentified article text is input to the 'summary' heading\n",
        "    else:\n",
        "        sections.update({'summary':text_body})\n",
        "\n",
        "    for s in ['background','response','epidemiology']:\n",
        "        if s not in sections:\n",
        "            sections.update({s:'None'})\n",
        "\n",
        "    #extract DON ID from url\n",
        "    DON = url.split('/item/')[-1]\n",
        "    #format date as day month year\n",
        "    dt = datetime.strptime(date,'%d %B %Y')\n",
        "    locate = where.strip()\n",
        "    #get iso codes from countries in title\n",
        "    #if no country iso found, return unknown\n",
        "    #usually means area specified as region instead of country, like 'Europe' or 'Latin America'\n",
        "    try:\n",
        "        #separate multi-country titles and get iso codes as a list\n",
        "        if ' and ' in locate:\n",
        "            iso = []\n",
        "            for i in locate.split(' and '):\n",
        "                for country, code in iso_dict.items():\n",
        "                    i = i.replace(country, code)\n",
        "                try:\n",
        "                    i = re.search('[A-Z]{3}',i.strip())[0]\n",
        "                    iso.append(i)\n",
        "                except Exception:\n",
        "                    #titles with 'multi-country' shortened to 'country' during earlier disease-country splitting\n",
        "                    for word, initial in {'the':'','of':'','Region':'','European':'Europe','African':'Africa','situation':''}.items():\n",
        "                        i = i.replace(word.lower(), initial.lower()).capitalize()\n",
        "                    iso.append(i)\n",
        "\n",
        "        #get single-country title iso codes\n",
        "        else:\n",
        "            for country, code in iso_dict.items():\n",
        "                locate = locate.replace(country, code)\n",
        "            try:\n",
        "                iso = re.search('[A-Z]{3}',locate.strip())[0]\n",
        "            except Exception:\n",
        "                for word, initial in {'the':'','of':'','Region':'','European':'Europe','African':'Africa','situation':''}.items():\n",
        "                    locate = locate.replace(word.lower(), initial.lower()).capitalize()\n",
        "                iso = locate\n",
        "            for i in [\"[\",\"]\",\"'\"]:\n",
        "                iso = iso.replace(i,'')\n",
        "    except TypeError:\n",
        "        iso = 'None'\n",
        "\n",
        "\n",
        "    #create unique ID using date of report and country iso code\n",
        "    new_id = f'{dt.year}_{dt.month}_{dt.day}_{iso}'\n",
        "\n",
        "    #write finished file that can be read by python as a dictionary with eval(file_text)\n",
        "    f = open(f'{path}/downloaded_articles/{dt.year}/{new_id}.txt','w')\n",
        "    lines = ['\"id\": ',f'\"{new_id}\"',',\\n','\"date\": ',f'\"{date}\"',',\\n', '\"disease\": ',f'\"{what}\"',',\\n',\n",
        "             '\"location\": ',f'\"{where}\"',',\\n','\"iso\": ',f'\"{iso}\"',',\\n','\"DON\": ',f'\"{DON}\"',',\\n',\n",
        "             '\"summary\": ',f'\"\"\"\\n{sections[\"summary\"].strip()}\\n\"\"\"',',\\n','\"background\": ',f'\"\"\"\\n{sections[\"background\"].strip()}\\n\"\"\"',',\\n',\n",
        "             '\"response\": ',f'\"\"\"\\n{sections[\"response\"].strip()}\\n\"\"\"',',\\n','\"epidemiology\": ',f'\"\"\"\\n{sections[\"epidemiology\"].strip()}\\n\"\"\"']\n",
        "    #write line by line\n",
        "    lines = ['{']+lines+['}']\n",
        "    f.writelines(lines)\n",
        "    f.close()\n",
        "    #print ID for troubleshooting\n",
        "    print(new_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b896b5b5-96b8-47af-848e-f8742feb8a04",
      "metadata": {
        "tags": [],
        "id": "b896b5b5-96b8-47af-848e-f8742feb8a04"
      },
      "outputs": [],
      "source": [
        "#example code to get all urls for range(of pages) from saved text files\n",
        "pages = {}\n",
        "for p in range(1,143):\n",
        "    f = open(f'{path}/pages/WHO_page_{p}.txt')\n",
        "    urls = [x.strip('\\n') for x in f.readlines()]\n",
        "    pages.update({f'page_{p}':urls})\n",
        "\n",
        "#example code to get article text for range(of pages)\n",
        "for v in pages.values():\n",
        "    for url in v:\n",
        "        get_article(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "121ddecc-0fc9-4ee5-9c00-ed7ec2d40ee9",
      "metadata": {
        "tags": [],
        "id": "121ddecc-0fc9-4ee5-9c00-ed7ec2d40ee9"
      },
      "outputs": [],
      "source": [
        "#example code to get all article urls from most recent url list\n",
        "with open(f'{path}/urls/all_{now}.txt', 'r') as f:\n",
        "    urls = f.read()\n",
        "\n",
        "#example code to get article text for all articles\n",
        "for u in urls.split('\\n'):\n",
        "  get_article(u)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}